#!/bin/bash
#SBATCH --account=aifac_l01_028
#SBATCH --job-name=scalability_test
#SBATCH --cpus-per-task=16    
#SBATCH --ntasks-per-node=1
#SBATCH --nodes=1       
#SBATCH --partition=boost_usr_prod
#SBATCH --qos=boost_qos_dbg
#SBATCH --time=20:00
#SBATCH --gpus-per-node=4
#SBATCH --gres=gpu:4
#SBATCH --threads-per-core=1
#SBATCH --output=/leonardo_scratch/large/userexternal/donutu00/logs/slurm_logs/megatron/%j.out
#SBATCH --error=/leonardo_scratch/large/userexternal/donutu00/logs/slurm_logs/megatron/%j.err


# For quick tests on max 2 nodes: #SBATCH --qos=boost_qos_dbg
# For large node numbers (> 64 nodes): add #SBATCH --qos=boost_qos_bprod
#
# Megatron-LM Slurm Script for Leonardo HPC
# Based on scripts from Sampo Pyysalo, Jenia Jitsev, Joerg Franke, Marianna Nezhurina, Thomas van Osch 
# WARNING: ADAPT corresponding paths carefully
# Usage:
#     sbatch pretrain-gpt-lrnd.sh


######################################################################
# ENVIRONMENT SETUP AND GENERAL CONFIGURATION
######################################################################

# Bash "strict mode"
# (see http://redsymbol.net/articles/unofficial-bash-strict-mode/)
set -euo pipefail

# Modules are needed with the container we are using.
module purge
module load python/3.11.7 nccl/2.22.3-1--gcc--12.2.0-cuda-12.2-spack0.22
# Avoid conflicts with $HOME/.local
export PYTHONUSERBASE=""

# Compilers in the container
export CC=gcc
export CXX=g++


######################################################################
# VARIABLE DEFINITIONS
######################################################################

export WANDB_ENTITY=oellm-team
WANDB_PROJECT="dense"
N_PARAMS="1.31"


# PATHS
export PROJECT_SPACE="$WORK/$USER"

if [ -z "$PROJECT_SPACE" ]; then
  echo "Error: PROJECT_SPACE is not set. Please set the project space path in this script. Example: export PROJECT_SPACE=/projects/0/prjsXXXX"
  exit 1
fi

MEGATRON_PATH="$PROJECT_SPACE/Megatron-LM"
OUTPUT_DIR="$SCRATCH/megatron"
CONTAINER=$PROJECT_SPACE/containers/megatron-torch-2.7-nvcr.25-04.sif
MEGATRON_CACHE_FOLDER="${OUTPUT_DIR}/megatron_cache"
CHECKPOINT_PATH="$OUTPUT_DIR/checkpoints"
TENSORBOARD_DIR="$OUTPUT_DIR/tensorboard/$SLURM_JOB_NAME-$SLURM_JOBID"
WANDB_DIR="$OUTPUT_DIR/wandb/$SLURM_JOB_NAME-$SLURM_JOBID"

mkdir -p "$OUTPUT_DIR" "$CHECKPOINT_PATH" "$MEGATRON_CACHE_FOLDER"   # This needs to exist

# DATA
DATA_PATH="$PROJECT_SPACE/datasets/FineWeb/fineweb-10BT_text_document" # gpt-2
# DATA_PATH="$PROJECT_SPACE/datasets/FineWeb/fineweb-10BT-llama3_text_document" # llama 3
# TOKENIZER_MODEL="gpt2"
TOKENIZER_MODEL="$PROJECT_SPACE/tokenizers/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e"
# TOKENIZER_MODEL="$PROJECT_SPACE/tokenizers/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/8c22764a7e3675c50d4c7c9a4edb474456022b16" # llama3

# Directories to map into container
BIND_DIRS="$OUTPUT_DIR,$PROJECT_SPACE,$MEGATRON_PATH"

# Apptainer cache
export APPTAINER_CACHEDIR="${MEGATRON_CACHE_FOLDER}/APPTAINER_CACHEDIR"
export APPTAINER_TMPDIR="${MEGATRON_CACHE_FOLDER}/APPTAINER_TMPDIR"
mkdir -p $APPTAINER_CACHEDIR $APPTAINER_TMPDIR


if [ "$SLURM_JOB_PARTITION" == "gpu_h100" ]; then
    # Known issue on Snellius. NCCL sometimes hangs during initialization.
    # Workaround: set NCCL_SOCKET_IFNAME to the ethernet interface for initialization.
    echo "Using H100 partition on Snellius, setting NCCL_SOCKET_IFNAME to eno2np0"
    export NCCL_SOCKET_IFNAME="eno2np0"
    SERVER="SNLSh"
    export WANDB_MODE=online

elif [ "$SLURM_JOB_PARTITION" == "gpu_a100" ]; then
    echo "Using A100 partition on Snellius, setting NCCL_SOCKET_IFNAME to eno1np0"
    export NCCL_SOCKET_IFNAME="eno1np0"
    SERVER="SNLSa"
    export WANDB_MODE=online

# ${SYSTEMNAME:-} expans to an empty string if SYSTEMNAME is not set
elif [[ "${SYSTEMNAME:-}" == juwelsbooster ]]; then
    echo "Using Juwels booster partition, setting GLOO_SOCKET_IFNAME to ib0"
    # Prevent Gloo not being able to communicate.
    export GLOO_SOCKET_IFNAME=ib0
    # Code works without this as well
    # export NCCL_SOCKET_IFNAME=ib0
    SERVER="JSC"

    # Note from https://apps.fz-juelich.de/jsc/hps/juwels/gpu-computing.html#gpu-visibility-affinity
    export CUDA_VISIBLE_DEVICES=0,1,2,3

    # Found as example here: https://apps.fz-juelich.de/jsc/hps/juwels/batchsystem.html#writing-a-batch-script
    # export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}    # gives error: libgomp: Invalid value for environment variable OMP_NUM_THREADS: 

    export WANDB_MODE=offline

elif [[ "$SLURM_CLUSTER_NAME" = leonardo ]]; then
    echo "Using Leonardo booster partition"
    SERVER="LRDN"
    export WANDB_MODE=offline
fi 


TOTAL_GPUS=$((SLURM_NNODES * SLURM_GPUS_PER_NODE))

# Needed for sequence paralellism
# (see https://github.com/NVIDIA/Megatron-LM/issues/533)
export CUDA_DEVICE_MAX_CONNECTIONS=1

export MASTER_ADDR=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
export MASTER_PORT=9999
export WORLD_SIZE=${SLURM_NTASKS:-$(( ${SLURM_GPUS_PER_NODE:-1} * ${SLURM_NNODES:-2} ))} # Note: only valid if ntasks==ngpus

# OMP THREADING
export OMP_NUM_THREADS=${SLURM_CPUS_PER_NODE:-2}

# This setting is reported to provide a performance improvement
# (https://arxiv.org/pdf/2408.14090v1) but as of April 2025 is causing
# training instability on LUMI with pipeline parallelism.
# (see https://github.com/spyysalo/lumi-fineweb-replication/issues/1)
#export NCCL_NCHANNELS_PER_PEER=32


# DEBUGGING, INCREASE VERBOSITY IN LOGS
# export MIOPEN_ENABLE_LOGGING=1
export PYTHONWARNINGS=ignore
# export TORCH_SHOW_CPP_STACKTRACES=1
# export NCCL_DEBUG=INFO
# export RCCL_KERNEL_COLL_TRACE_ENABLE=1
# export NCCL_DEBUG_SUBSYS=ALL
# export NCCL_DEBUG_FILE=$OUTPUT_DIR/nccl-debug-${SLURM_JOB_NAME}-${SLURM_JOBID}.log #Move verbose nccl logging to its own file
export NVTE_DEBUG=0
export NVTE_DEBUG_LEVEL=0

######################################################################
#
# MODEL AND PRETRAINING CONFIGURATION
#
# This section sets variables that define the model and pretraining
# configuration. These mostly correspond to command-line arguments to
# Megatron-LM/pretrain_gpt.py, and when they do, the names should
# match (e.g. the variable $GLOBAL_BATCH_SIZE gets passed as
# --global-batch-size). This script is intended to be configurable by
# redefining these variables.
#
######################################################################

# MODEL
# 1,713,735,680 (1.7B) parameters
NUM_LAYERS=24
HIDDEN_SIZE=2048
FFN_HIDDEN_SIZE=$((4*HIDDEN_SIZE))
NUM_ATTENTION_HEADS=32
NUM_QUERY_GROUPS=32    # No GQA when NUM_QUERY_GROUPS=NUM_ATTENTION_HEADS
TIE_WORD_EMBEDDINGS=0    # 1: tie, 0: untie
INIT_METHOD_STD=0.02
SEQ_LENGTH=4096
ROTARY_BASE=10000    # Default, recommend larger for higher seq len

# PARALLELISM
PIPELINE_MODEL_PARALLEL_SIZE=1
TENSOR_MODEL_PARALLEL_SIZE=1
CONTEXT_PARALLEL_SIZE=1
NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE=1
PROFILE=0

# OPTIMIZER
ADAM_BETA1=0.9
ADAM_BETA2=0.95
ADAM_EPS=1e-8
LR=3e-4
MIN_LR=3e-5
LR_WARMUP_ITERS=500
CLIP_GRAD=1.0
WEIGHT_DECAY=1e-1

# TRAINING
FSDP=0
MICRO_BATCH_SIZE=2         # previously 4 (which works on H100, but not on A100) because of OOM
GLOBAL_BATCH_SIZE=$((${MICRO_BATCH_SIZE} * ${TOTAL_GPUS}))   # previously 64.
RECOMPUTATION=0
TRAIN_TOKENS=350_000_000_000    # TRAIN_SAMPLES computed from this
TRAIN_STEPS=500    

# SAVING AND EVALUATION
LOG_INTERVAL=1
SAVE_INTERVAL=5000
EVAL_INTERVAL=5000
EVAL_ITERS=10000

######################################################################
#
# DERIVED CONFIGURATION SETTINGS
#
# The following settings are derived from the configuration above.
# Do set these directly, as they will be overwritten here.
#
######################################################################

# Check that variables are not set (sanity)
confirm_unset() {
    local varname="$1"
    if [ -n "${!varname+x}" ]; then
	echo "Error: variable '$varname' should not be set." >&2
	exit 1
    fi
}
confirm_unset "TRAIN_SAMPLES"
confirm_unset "LR_WARMUP_SAMPLES"
confirm_unset "LR_DECAY_SAMPLES"

# Calculate TRAIN_SAMPLES from TRAIN_TOKENS
TRAIN_TOKENS=${TRAIN_TOKENS//_}    # drop "_" for bash math
# TRAIN_SAMPLES=$((TRAIN_TOKENS/SEQ_LENGTH))

# Set LR_WARMUP_SAMPLES and LR_DECAY_SAMPLES and based LR_WARMUP_ITERS
# and TRAIN_SAMPLES
LR_WARMUP_SAMPLES=$((LR_WARMUP_ITERS*GLOBAL_BATCH_SIZE))
# LR_DECAY_SAMPLES=$TRAIN_SAMPLES

######################################################################
#
# BUILDING COMMAND-LINE ARGUMENTS
#
# The following builds the command-line arguments for
# Megatron-LM/pretrain_gpt.py based on the variables defined above
# (and optionally in any config given to the script). Note that some
# arguments that are not expected to vary are hard-coded here.
#
######################################################################

DATA_ARGS=(
    --data-path "$DATA_PATH"
    --data-cache-path "$MEGATRON_CACHE_FOLDER"
    --tokenizer-type HuggingFaceTokenizer
    --tokenizer-model "$TOKENIZER_MODEL"
    --make-vocab-size-divisible-by 128
    --dataloader-type single
    --num-workers 2   # Some issues with this, lower values are safer
)

MODEL_ARGS=(
    --num-layers $NUM_LAYERS
    --hidden-size $HIDDEN_SIZE
    --ffn-hidden-size $FFN_HIDDEN_SIZE
    --num-attention-heads $NUM_ATTENTION_HEADS
)

if [ "$NUM_QUERY_GROUPS" != "$NUM_ATTENTION_HEADS" ]; then
    MODEL_ARGS+=(
        --group-query-attention
        --num-query-groups $NUM_QUERY_GROUPS
    )
fi

if [ "$TIE_WORD_EMBEDDINGS" = "0" ]; then
    MODEL_ARGS+=(
	--untie-embeddings-and-output-weights
    )
fi

if [ "$FSDP" = "1" ]; then
    PARALLEL_ARGS=(
	--use-torch-fsdp2
    )
else
    PARALLEL_ARGS=(
	--tensor-model-parallel-size $TENSOR_MODEL_PARALLEL_SIZE
	--pipeline-model-parallel-size $PIPELINE_MODEL_PARALLEL_SIZE
	--context-parallel-size $CONTEXT_PARALLEL_SIZE
	--sequence-parallel
	--use-distributed-optimizer
    )
fi

if [ "$PROFILE" = "1" ]; then
    PROFILE_ARGS=(
	--use-pytorch-profiler
	--profile-ranks 0
	--profile-step-start 5
	--profile-step-end 7
    )
else
    PROFILE_ARGS=()
fi

MODEL_ARGS+=(
    # --use-flash-attn
    --attention-softmax-in-fp32
    --max-position-embeddings $((SEQ_LENGTH - 1024))
    --seq-length $((SEQ_LENGTH - 1024))
    --position-embedding-type rope
    --rotary-base $ROTARY_BASE
    --disable-bias-linear
    --init-method-std $INIT_METHOD_STD
    --attention-dropout 0.0
    --hidden-dropout 0.0
    --normalization LayerNorm # default: RMSNorm
    --micro-batch-size $MICRO_BATCH_SIZE
    # --global-batch-size $GLOBAL_BATCH_SIZE
    # --train-samples $TRAIN_SAMPLES
    --bf16
    # --swiglu # default was swiglu
    # --no-async-tensor-model-parallel-allreduce
    # --no-masked-softmax-fusion
    # --no-gradient-accumulation-fusion
    # --no-bias-dropout-fusion
    --no-rope-fusion    # buggy on AMD, do not enable without validating
    --distributed-timeout-minutes 30
    --overlap-grad-reduce
)

OPTIMIZER_ARGS=(
    --optimizer adam
    --adam-beta1 $ADAM_BETA1
    --adam-beta2 $ADAM_BETA2
    --adam-eps $ADAM_EPS
    --lr $LR
    --min-lr $MIN_LR
    --lr-decay-style cosine
    # --lr-decay-samples $LR_DECAY_SAMPLES
    # --lr-warmup-samples $LR_WARMUP_SAMPLES
    --lr-warmup-fraction 0.01
    --clip-grad $CLIP_GRAD
    --weight-decay $WEIGHT_DECAY
    --train-iters $TRAIN_STEPS
)


DATASET=fineweb
MODEL=megatron-ar
SL_NAME=""
GAS_NAME=""
if [ "${SEQ_LENGTH:-}" ]; then
    SL_NAME="-${SEQ_LENGTH}sl"
fi
if [ "${GAS:-}" ]; then
    GAS_NAME="-${GAS}gas"
fi

JOB_NAME="${SERVER}-${SLURM_JOB_ID}-${N_PARAMS}B-${TOTAL_GPUS}gpu-${MICRO_BATCH_SIZE}mbs-${DATASET}-${MODEL}${SL_NAME}${GAS_NAME}"
echo "Job name: $JOB_NAME"


if [ "${WANDB_API_KEY:-}" ]; then
    
    WANDB_ARGS=(
        --wandb-project $WASNDB_PROJECT
        --wandb-exp-name $JOB_NAME
        --wandb-save-dir $WANDB_DIR
    )
    export WANDB_DATA_DIR=$CACHE_DIR/wandb
    export WANDB_CACHE_DIR=$CACHE_DIR/wandb-cache
mkdir -p $WANDB_CACHE_DIR

    mkdir -p $WANDB_DATA_DIR
else
    WANDB_ARGS=()
fi


OUTPUT_ARGS=(
    --eval-interval $EVAL_INTERVAL
    --eval-iters $EVAL_ITERS
    --tensorboard-dir "$TENSORBOARD_DIR"
    --tensorboard-queue-size 5
    --log-throughput
    --log-timers-to-tensorboard
    --log-progress
    --log-interval $LOG_INTERVAL
    --tensorboard-log-interval 1

)

# Interleaved pipeline scheduling is only possible with pipeline
# parallel degree > 1.
if [ $PIPELINE_MODEL_PARALLEL_SIZE -gt 1 ] && [ $NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE -gt 1 ]; then
    PARALLEL_ARGS+=(
	--num-layers-per-virtual-pipeline-stage $NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE
    )
fi

if [ "$RECOMPUTATION" = "1" ]; then
    MODEL_ARGS+=(
	--recompute-activations
	--recompute-granularity selective
    )
fi

CHECKPOINT_ARGS=(
    --ckpt-format torch    # "legacy" checkpoints; torch_dist is crashing
#     --async-save    # requires --ckpt-format torch_dist
    --save "$CHECKPOINT_PATH"
    --save-interval $SAVE_INTERVAL
)



######################################################################
#
# Run the command through the launch script with srun.
# Note that any node-specific setup needs to go into the launch script.
#
######################################################################

# Command
COMMAND=" ${MEGATRON_PATH}/pretrain_gpt.py \
    "${MODEL_ARGS[@]}" \
    "${OPTIMIZER_ARGS[@]}" \
    "${PARALLEL_ARGS[@]}" \
    "${OUTPUT_ARGS[@]}" \
    "${CHECKPOINT_ARGS[@]}" \
    "${DATA_ARGS[@]}" \
    "${PROFILE_ARGS[@]}" \
    "${WANDB_ARGS[@]}" \
"

echo "START $SLURM_JOBID: $(date)"
echo "SLURM_NNODES: $SLURM_NNODES"


LAUNCHER="singularity exec \
    --nv \
    --bind $BIND_DIRS \
    $CONTAINER \
    python -u -m torch.distributed.run \
        --nproc-per-node $SLURM_GPUS_PER_NODE \
        --nnodes $SLURM_NNODES \
        --rdzv_endpoint $MASTER_ADDR:$MASTER_PORT \
        --rdzv_backend static \
        --max_restarts 0 \
        --tee 3"

srun \
    --wait=60 \
    --cpus-per-task=$SLURM_CPUS_PER_TASK \
    --threads-per-core=1 \
    --kill-on-bad-exit=1 \
    --jobid $SLURM_JOB_ID \
    bash -c "$LAUNCHER --node_rank \$SLURM_PROCID --role \$SLURMD_NODENAME: $COMMAND"

echo "END $SLURM_JOBID: $(date)"
